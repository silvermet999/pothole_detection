{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40e1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import PIL.Image as Image\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89501518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='best-fp16.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "# interpreter.set_tensor(input_details[0]['index'], input_image_dims)\n",
    "\n",
    "interpreter.invoke()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec143a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 640, 640,   3], dtype=int32),\n",
       "  'shape_signature': array([  1, 640, 640,   3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 640, 640,   3], dtype=int32),\n",
       "  'shape_signature': array([  1, 640, 640,   3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf44c95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 617,\n",
       "  'shape': array([    1, 25200,     6], dtype=int32),\n",
       "  'shape_signature': array([    1, 25200,     6], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 617,\n",
       "  'shape': array([    1, 25200,     6], dtype=int32),\n",
       "  'shape_signature': array([    1, 25200,     6], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15665c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details[0][\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94c54284",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(\u001b[43moutput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m boxes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(\u001b[43moutput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m boxes \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = interpreter.get_tensor(output_details[1][\"index\"])\n",
    "boxes = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "scores = interpreter.get_tensor(output_details[2][\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4202b27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.6405147e-01, 7.8653115e-01, 2.6819330e-01, 1.7300276e-01,\n",
       "       1.2599843e-05, 9.9998343e-01], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_tensor(output_details[0]['index'])[0][23000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f259a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "im = Image.open(\"data/110.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_im = im.resize((192, 192))\n",
    "res_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = np.array(res_im) / 127.5 - 1.0\n",
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_u = (input_image).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f938d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image_dims = np.expand_dims(input_image_u, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb210aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Define function for inferencing with TFLite model and displaying results\n",
    "\n",
    "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
    "\n",
    "  # Grab filenames of all images in test folder\n",
    "    images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
    "\n",
    "  # Load the label map into memory\n",
    "    with open(lblpath, 'r') as f:\n",
    "        labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "  # Load the Tensorflow Lite model into memory\n",
    "    interpreter = Interpreter(model_path=modelpath)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "  # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "\n",
    "    float_input = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "    input_mean = 127.5\n",
    "    input_std = 127.5\n",
    "\n",
    "  # Randomly select test images\n",
    "    images_to_test = random.sample(images, num_test_images)\n",
    "\n",
    "  # Loop over every image and perform detection\n",
    "    for image_path in images_to_test:\n",
    "\n",
    "      # Load image and resize to expected shape [1xHxWx3]\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        imH, imW, _ = image.shape\n",
    "        image_resized = cv2.resize(image_rgb, (width, height))\n",
    "        input_data = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "        if float_input:\n",
    "              input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "      # Perform the actual detection by running the model with the image as input\n",
    "        interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "        interpreter.invoke()\n",
    "\n",
    "      # Retrieve detection results\n",
    "        boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\n",
    "        classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\n",
    "        scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\n",
    "\n",
    "        detections = []\n",
    "\n",
    "      # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "        for i in range(len(scores)):\n",
    "            if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
    "\n",
    "              # Get bounding box coordinates and draw box\n",
    "              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n",
    "                ymin = int(max(1,(boxes[i][0] * imH)))\n",
    "                xmin = int(max(1,(boxes[i][1] * imW)))\n",
    "                ymax = int(min(imH,(boxes[i][2] * imH)))\n",
    "                xmax = int(min(imW,(boxes[i][3] * imW)))\n",
    "\n",
    "                cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
    "\n",
    "              # Draw label\n",
    "                object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
    "                label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
    "                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
    "                label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
    "                cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
    "                cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
    "\n",
    "                detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
    "\n",
    "\n",
    "      # All the results have been drawn on the image, now display the image\n",
    "        if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n",
    "            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12,16))\n",
    "            plt.imshow(image)\n",
    "            plt.show()\n",
    "\n",
    "      # Save detection results in .txt files (for calculating mAP)\n",
    "        elif txt_only == True:\n",
    "\n",
    "        # Get filenames and paths\n",
    "            image_fn = os.path.basename(image_path)\n",
    "            base_fn, ext = os.path.splitext(image_fn)\n",
    "            txt_result_fn = base_fn +'.txt'\n",
    "            txt_savepath = os.path.join(savepath, txt_result_fn)\n",
    "\n",
    "        # Write results to text file\n",
    "        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n",
    "        with open(txt_savepath,'w') as f:\n",
    "            for detection in detections:\n",
    "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
    "\n",
    "    return\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd773b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMAGES='potholes'   # Path to test images folder\n",
    "PATH_TO_MODEL='best-fp16.tflite'   # Path to .tflite model file\n",
    "PATH_TO_LABELS='labelmap.txt'   # Path to labelmap.txt file\n",
    "min_conf_threshold=0.5   # Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
    "images_to_test = 10   # Number of images to run detection on\n",
    "\n",
    "# Run inferencing function!\n",
    "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpreter(model_path=modelpath)\n",
    "interpreter.allocate_tensors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
